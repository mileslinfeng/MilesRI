# D:\usstocks\server\tools\earnings_calendar_fetch.py
import os
import sys
import json
import time
import requests
from datetime import datetime, timedelta

def log(msg):
    """è¾“å‡ºåˆ° stderrï¼ˆNode ä¸ä¼šè§£æè¿™é‡Œçš„å†…å®¹ï¼‰"""
    sys.stderr.write(msg + "\n")
    sys.stderr.flush()


FMP_KEY  = os.getenv("FMP_API_KEY") or os.getenv("FMP_KEY") or "z1m4vMNiLtZ1oXbdGJIulSpbMxGfLqvx"
FINN_KEY = os.getenv("FINNHUB_KEY") or os.getenv("FINNHUB_TOKEN") or "d46d1epr01qgc9es8a40d46d1epr01qgc9es8a4g"
log(f"ğŸ” Keys loaded: FMP={bool(FMP_KEY)} FINN={bool(FINN_KEY)}")

CACHE_FILE = "calendar_cache.json"
CACHE_TTL = 60 * 30  # 30åˆ†é’Ÿ

log("ğŸš€ [fetch_all] å¼€å§‹æ‰§è¡Œ")


def to_iso(d):
    try:
        return datetime.fromisoformat(str(d)[:10]).strftime("%Y-%m-%d")
    except Exception:
        return None


def safe_num(v):
    try:
        return float(v)
    except:
        return None


def cache_load():
    if os.path.exists(CACHE_FILE):
        if time.time() - os.path.getmtime(CACHE_FILE) < CACHE_TTL:
            try:
                return json.load(open(CACHE_FILE, "r", encoding="utf-8"))
            except:
                pass
    return None


def cache_save(data):
    json.dump(data, open(CACHE_FILE, "w", encoding="utf-8"), indent=2)


def fetch_fmp(from_date, to_date):
    url = f"https://financialmodelingprep.com/api/v3/earning_calendar?from={from_date}&to={to_date}&apikey={FMP_KEY}"
    log(f"ğŸ“… Fetching FMP: {url}")
    r = requests.get(url, timeout=15)
    if r.status_code != 200:
        log(f"âŒ FMP Error {r.status_code}")
        return []
    try:
        data = r.json()
    except Exception as e:
        log(f"âš ï¸ JSON Decode Error (FMP): {e}")
        return []

    if not isinstance(data, list):
        return []
    out = []
    for d in data:
        out.append({
            "symbol": d.get("symbol"),
            "date": to_iso(d.get("date") or d.get("filingDate")),
            "eps": safe_num(d.get("eps") or d.get("epsEstimate") or d.get("estimatedEps")),
            "revenue": safe_num(d.get("revenue") or d.get("revenueActual")),
            "revenueEstimate": safe_num(d.get("revenueEstimate") or d.get("estimatedRevenue")),
            "time": "After Close" if (d.get("hour") or d.get("time")) == "amc" else ("Before Open" if (d.get("hour") or d.get("time")) == "bmo" else "N/A"),
            "source": "FMP",
            "marketCap": safe_num(d.get("marketCap")),
            "price": safe_num(d.get("price")),
            "sector": d.get("sector") or "N/A"
        })

    log(f"âœ… FMP è¿”å› {len(out)} æ¡è®°å½•")
    log(f"ğŸ§¾ FMP åŸå§‹æ•°æ®é¢„è§ˆ:")
    for i, d in enumerate(out[:20]):
        log(f"{i+1}. {json.dumps(d, ensure_ascii=False)}")
    return out


def fetch_finnhub(from_date, to_date):
    url = f"https://finnhub.io/api/v1/calendar/earnings?from={from_date}&to={to_date}&token={FINN_KEY}"
    log(f"ğŸ“… Fetching Finnhub: {url}")
    r = requests.get(url, timeout=15)
    if r.status_code != 200:
        log(f"âŒ Finnhub Error {r.status_code}")
        return []
    try:
        data = r.json()
    except Exception as e:
        log(f"âš ï¸ JSON Decode Error (Finnhub): {e}")
        return []

    try:
        items = data.get("earningsCalendar", [])
        log(f"âœ… Finnhub è¿”å› {len(items)} æ¡è®°å½•")
        log("ğŸ§¾ Finnhub åŸå§‹æ•°æ®é¢„è§ˆï¼ˆå‰20æ¡ï¼‰ï¼š")
        for i, d in enumerate(items[:20]):
            log(f"{i+1}. {json.dumps(d, ensure_ascii=False)}")

        out = []
        for d in items:
            out.append({
                "symbol": d.get("symbol"),
                "date": to_iso(d.get("date")),
                "eps": safe_num(d.get("epsEstimate")),
                "revenue": safe_num(d.get("revenueActual")),
                "revenueEstimate": safe_num(d.get("revenueEstimate")),
                "time": "After Close" if (d.get("hour") or d.get("time")) == "amc" else ("Before Open" if (d.get("hour") or d.get("time")) == "bmo" else "N/A"),
                "source": "Finnhub",
                "marketCap": safe_num(d.get("marketCapitalization")),
                "price": safe_num(d.get("close")),
                "sector": d.get("sector") or "N/A"
            })
        return out
    except Exception as e:
        log("âŒ Finnhub Parse Error:" + str(e))
        return []

# 


def fetch_quote(symbol):
    url = f"https://financialmodelingprep.com/api/v3/profile/{symbol}?apikey={FMP_KEY}"
    try:
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            j = r.json()
            if isinstance(j, list) and len(j) > 0:
                return {
                    "price": safe_num(j[0].get("price")),
                    "marketCap": safe_num(j[0].get("mktCap")),
                    "sector": j[0].get("sector")
                }
    except:
        pass
    return {}

def group_by_time(data):
    log("ğŸ§© è¿›å…¥ group_by_time()ï¼Œå…±æ”¶åˆ° %d æ¡è®°å½•" % len(data))
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    week_ahead = today + timedelta(days=7)
    month_ahead = today + timedelta(days=30)

    groups = {"yesterday": [], "today": [], "thisWeek": [], "thisMonth": []}
    invalid_count = 0

    for d in data:
        date_str = d.get("date")
        if not date_str:
            invalid_count += 1
            continue
        try:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
        except Exception as e:
            invalid_count += 1
            log(f"âš ï¸ æ— æ•ˆæ—¥æœŸæ ¼å¼: {date_str} ({e})")
            continue

        if dt == yesterday:
            groups["yesterday"].append(d)
        elif dt == today:
            groups["today"].append(d)
        elif today < dt <= week_ahead:
            groups["thisWeek"].append(d)
        elif week_ahead < dt <= month_ahead:
            groups["thisMonth"].append(d)

    log(f"âœ… group_by_time() å®Œæˆï¼Œæœ‰æ•ˆè®°å½• {len(data)-invalid_count} æ¡ï¼Œä¸¢å¼ƒ {invalid_count} æ¡")
    for k in groups:
        log(f"  â””â”€ {k}: {len(groups[k])} æ¡")
        groups[k] = sorted(groups[k], key=lambda x: x["date"])
    return groups


def fetch_all():
    log("ğŸš€ å¼€å§‹ fetch_all() æµç¨‹")

    cache = cache_load()
    if cache:
        log("ğŸ“ ä½¿ç”¨ç¼“å­˜æ•°æ®")
        return cache

    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    month_ahead = today + timedelta(days=30)
    from_date = yesterday.strftime("%Y-%m-%d")
    to_date = month_ahead.strftime("%Y-%m-%d")
    log(f"ğŸ“… æ—¥æœŸèŒƒå›´: {from_date} â†’ {to_date}")

    # === å°è¯•ä» FMP æ‹‰å– ===
    data = fetch_fmp(from_date, to_date)
    log(f"ğŸ“Š ä» FMP æ‹¿åˆ° {len(data)} æ¡è®°å½•")

    # === è‹¥ FMP ä¸ºç©ºï¼Œå†ä» Finnhub æ‹‰å– ===
    if not data:
        data = fetch_finnhub(from_date, to_date)
        log(f"ğŸ“Š ä» Finnhub æ‹¿åˆ° {len(data)} æ¡è®°å½•")

        # âœ… æ‰“å°å‰20æ¡è®°å½•é¢„è§ˆ
        try:
            log("ğŸ§¾ Finnhub æ•°æ®å‰ 20 æ¡å†…å®¹ï¼š")
            for i, d in enumerate(data[:20]):
                log(f"{i+1}. {json.dumps(d, ensure_ascii=False)}")
        except Exception as e:
            log(f"âš ï¸ æ‰“å°é¢„è§ˆæ—¶å‡ºé”™: {e}")

    # === è‹¥ä¸¤è€…éƒ½ä¸ºç©º ===
    if not data:
        log("âš ï¸ ä¸¤ä¸ªæ•°æ®æºéƒ½æ— æ•°æ®ï¼Œä½¿ç”¨ mock æ•°æ®")
        data = [
            {"symbol": "AAPL", "date": today.strftime("%Y-%m-%d"), "eps": 1.2, "revenue": 9e10, "revenueEstimate": 9.2e10, "time": "After Close", "source": "Mock"},
            {"symbol": "MSFT", "date": today.strftime("%Y-%m-%d"), "eps": 2.4, "revenue": 7.8e10, "revenueEstimate": 8.0e10, "time": "Before Open", "source": "Mock"},
            {"symbol": "NVDA", "date": (today + timedelta(days=5)).strftime("%Y-%m-%d"), "eps": 1.05, "revenue": 4.6e10, "revenueEstimate": 4.8e10, "time": "After Close", "source": "Mock"},
        ]



    # === æ”¹è¿›ç‰ˆ yfinance æ‰¹é‡è¡¥å…¨ ===
    import yfinance as yf
    from math import ceil

    log(f"ğŸ” ä½¿ç”¨ yfinance æ‰¹é‡è¡¥å…¨å¸‚åœºä¿¡æ¯ï¼Œå…± {len(data)} æ¡")
    symbols = list({d["symbol"] for d in data if d.get("symbol")})
    batch_size = 50  # â¬…ï¸ é¦–æ¬¡æŠ“å–æ›´ç¨³ä¸€äº›
    yf_data = {}

    total_batches = ceil(len(symbols) / batch_size)
    log(f"ğŸ“¦ yfinance æ‰¹æ¬¡æ•°: {total_batches}ï¼ˆæ¯æ‰¹ {batch_size} æ”¯ï¼‰; symbols={len(symbols)}")


    for i in range(0, len(symbols), batch_size):
        batch = symbols[i:i + batch_size]
        batch_no = (i // batch_size) + 1
        log(f"â³ yfinance æ‰¹ {batch_no}/{total_batches}ï¼š{batch[0]} ~ {batch[-1]}")

        try:
            tickers = yf.Tickers(" ".join(batch))
            for sym, obj in tickers.tickers.items():
                info = getattr(obj, "info", {})
                yf_data[sym] = {
                    "price": safe_num(info.get("currentPrice")),
                    "marketCap": safe_num(info.get("marketCap")),
                    "sector": info.get("sector") or "N/A",
                }
            log(f"âœ… ç¬¬ {batch_no} æ‰¹å®Œæˆï¼Œç´¯è®¡è·å– {len(yf_data)} æ¡")
        except Exception as e:
            log(f"âš ï¸ ç¬¬ {batch_no} æ‰¹å¤±è´¥: {e}")
            continue
        log(f"âœ… æ‰¹ {batch_no} å®Œæˆï¼Œå½“å‰ç´¯è®¡ {len(yf_data)} æ¡ï¼ˆæœ¬æ‰¹ {len(batch)}ï¼‰")
    log(f"âœ… yfinance å…¨éƒ¨å®Œæˆï¼Œå…±è¿”å› {len(yf_data)} æ¡å…¬å¸ä¿¡æ¯")

    # === åˆå¹¶è¡¥å…¨æ•°æ® ===
    filled = 0
    for d in data:
        sym = d.get("symbol")
        if sym in yf_data:
            d.update(yf_data[sym])
            filled += 1
    log(f"âœ… yfinance è¡¥å…¨ç»“æŸï¼Œå…±æ›´æ–° {filled} æ¡/æ€» {len(data)} æ¡")



    log(f"ğŸ§® è¿›å…¥åˆ†ç»„å‰æ•°æ®é‡: {len(data)}")
    grouped = group_by_time(data)
    log("ğŸ’¾ å‡†å¤‡å†™å…¥ç¼“å­˜æ–‡ä»¶ calendar_cache.json")


    # âœ… è½¬æ¢æˆå•ä¸€æ•°ç»„ç»“æ„ï¼ˆé€‚é…å‰ç«¯ï¼‰
    merged = []
    for k in grouped:
        merged.extend(grouped[k])

    cache_save(merged)
    log(f"ğŸ fetch_all() ç»“æŸï¼Œæœ€ç»ˆè¿”å› {len(merged)} æ¡ç»Ÿä¸€è®°å½•")
    return merged


if __name__ == "__main__":
    try:
        print("âœ… Python è„šæœ¬å¼€å§‹æ‰§è¡Œ", file=sys.stderr)
        merged = fetch_all()
        print("âœ… fetch_all å®Œæˆ", file=sys.stderr)
        print(json.dumps(merged, ensure_ascii=False))
    except Exception as e:
        print(json.dumps({"error": str(e)}, ensure_ascii=False))

